# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/06 evaluation.transcription.ipynb.

# %% auto 0
__all__ = ['get_latest_checkpoint', 'get_latest_model', 'transcribe', 'update_pbar']

# %% ../../nbs/06 evaluation.transcription.ipynb 2
from pathlib import Path
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn
import torch
from torchtext.vocab import Vocab
from tqdm import tqdm
import math

from ..data.download import load_config, get_latest_assembly_path
from ..training.transcription.generation import get_genes, get_mrna
from llm_mito_scanner.training.transcription.train import translate, Seq2SeqTransformer,\
    get_vocab, get_text_transform, set_vocab_idx

# %% ../../nbs/06 evaluation.transcription.ipynb 11
def get_latest_checkpoint(checkpoint_path: Path) -> dict:
    checkpoints = pd.DataFrame(list(checkpoint_path.glob("epoch-*-model.pt")), columns=['path'])
    checkpoints.loc[:, 'checkpoint'] = checkpoints.path.apply(lambda p: p.stem.split("-")[1]).astype(int)
    checkpoints.sort_values("checkpoint", inplace=True, ascending=True)
    latest_checkpoint_path = checkpoints.iloc[-1, 0]
    checkpoint = torch.load(latest_checkpoint_path)
    return checkpoint


def get_latest_model(
        vocab: Vocab, 
        state_dict: dict,
        encoder_layers: int = 1, 
        decoder_layers: int = 1,
        embedding_size: int = 32,
        nheads: int = 4,
        feed_forward_dim: int = 32,
        device: str = torch.device('cuda' if torch.cuda.is_available() else 'cpu')) -> Seq2SeqTransformer:
    SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = len(vocab)
    # Define model
    model = Seq2SeqTransformer(
        encoder_layers, decoder_layers, embedding_size,
        nheads, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, feed_forward_dim)
    model.load_state_dict(state_dict)
    model.to(device)
    return model

# %% ../../nbs/06 evaluation.transcription.ipynb 18
def transcribe(
        gene: list[str], 
        model: Seq2SeqTransformer, 
        vocab: Vocab, 
        length: int = 64, 
        pbar_position: int = 0) -> str:
    num_batches = max(1, (len(gene) // length) + 1)
    transcribed_tokens = []
    batch_pbar = tqdm(total=num_batches, 
        ncols=80, 
        leave=False, 
        miniters=5, 
        desc="Translating", 
        position=pbar_position)
    batch_range = range(0, num_batches)
    gene_length = len(gene)
    for i in batch_range:
        start = i * length
        end = min(start + length, gene_length)
        gene_batch = gene[start: end]
        transcribed_gene_batch = translate(model=model, vocab=vocab, src_sentence=gene_batch)
        transcribed_gene_batch = transcribed_gene_batch.split(",")
        transcribed_gene_batch = [s for s in transcribed_gene_batch if len(s) > 0]
        transcribed_tokens.extend(transcribed_gene_batch)
        batch_pbar.update(1)
    batch_pbar.close()
    return ",".join(transcribed_tokens)

# %% ../../nbs/06 evaluation.transcription.ipynb 23
def update_pbar(result, pbar):
    pbar.update(1)
    return result
